# DRAFT! Don't start yet

# P2 (3% of grade): gRPC and Containers

## Overview

In this project, you'll create a multi-container application for
looking up the addresses of houses in Madison with a given zipcode.
One set of containers will host the data and provide access via gRPC.
Their functionality is identical so that your application can continue
to operate even if one container fails.  Another set of containers
will provide an HTTP interface to the data.  This second set won't
actually store the original data, but will communicate with the first
set of containers to get the data necessary to answer queries.  The
second set of containers will have built-in LRU caches to reduce load
on the storage containers.  You will do this project using,
[Aider](https://aider.chat/), and will write prompts to help generate
code.  However, you are responsible for any code generated by Aider
(you must check it for correctness and do additional reading as
necessary to understand any code produced)

Learning objectives:
* communicate via gRPC
* tolerate failures with replication and retries
* implement an LRU cache
* prompt Aider to generate code on your behalf

Before starting, please review the [general project directions](../projects.md).

## AI Usage

You will do this project with Aider, a terminal-based AI-assisted
coding tool.  Similar tools are Claude Code, Codex, Gemini CLI.

Fill in question answers about your interactions with Aider as you go
in [`ai.md`](ai.md).

To install Aider, use pip to install the Aider installer program
(perhaps in a virtual env): `pip3 install aider-install`.  Then, run
the installer itself: `aider-install`.

Aider can be use in combination with different AI models.  For this
project, you are required to use `gemini-2.5-flash`.  To get access:
* go to https://aistudio.google.com/
* create an API key, and copy it
* store the key in an environment variable, like this: `export GEMINI_API_KEY="your-api-key-here"`.  You may want to put this in `~/.bashrc` so it runs with every new bash session (or whatever file is equivalent if you are using a different shell)
* follow the directions for [section 1](https://canvas.wisc.edu/courses/478879/discussion_topics/2237793) or [section 2](https://canvas.wisc.edu/courses/478879/discussion_topics/2237941) to link it to your Google Cloud credits.

We provide more detailed documentation about aider [here](../aider_intro).

Keep an eye on your credit consumption so you don't use more than $5 for this project, as we will need credits for other projects too.  It probably won't be difficult to complete the project in <$5, so you will likely have credits to experiment as well, if you like.  If you are running too low on credits, you will need to start writing more code manually.

After `cd`ing to your the directory where you cloned the repo for your project, start Aider like this:

```
aider --model gemini/gemini-2.5-flash --no-gitignore
```

You ran type `/help` to learn about different commands.  Most importantly:

* `/code` switch to coding mode, where you can prompt it to modify files on your behalf
* `/add` to give it permission to edit an additional file
* `/ask` to ask questions about the code without making changes

As you ask questions to Aider, your prompts will be recorded in
`./.aider.input.history`, and the prompts along with answers will be
stored in `./.aider.chat.history.md` (you need to run `ls -al` to see
these files).  You will need to commit and push these files as part of
your final submission (normally Aider tries to exclude them from
commits, which is why you must launch with `--no-gitignore`).

### Requirements

DOs:
- Use gemini-2.5-flash
- Break your work into many prompts (to get full credit, we expect at least 10 total commits, with at least 5 of those co-authored by Aider according to your git history)
- Write short prompts, at most ~200 words
- Express prompts using your own phrasing
- Write code manually on occasion when Aider is struggling and you can do things yourself faster
- Submit (commit+push) the Aider history files described above

DON'Ts:
- Use models others than Use gemini-2.5-flash
- Copy/paste some or all of the text of the project spec (we'll run simularity detection between your prompts and our spec).  Exceptions: you may copy starter code and names, like "<PROJECT>-dataset-1:5000" or "grpc.RpcError"
- Submit code you don't understand (either prompt Aider to use other approaches you are familiar with, or read on your own to understand Aider-generated code)

## Introduction
You'll need to write code and Dockerfiles to start 5 containers like this:

<img src="arch.png" width=600>

Take a look at the provided Docker compose file (you may not modify
it).  Note that there are two services, "cache" with 3 replicas and
"dataset" with 2 replicas.  The cache replicas will forward random
ports on the VM (probably not 8000-8002) to port 8080 inside the
containers.

You should have Dockerfiles named "Dockerfile.cache" and "Dockerfile.dataset" that we can build like this to produce the Docker images for these two services:

```
docker build . -f Dockerfile.cache -t p2-cache
docker build . -f Dockerfile.dataset -t p2-dataset
```

Note that the compose file assumes there is a "PROJECT" environment
variable.  You can set it to p2 in your environment:

```
export PROJECT=p2
```

Whatever this is set to will be a prefix for the container names.  For
example, if it is "abc", your first cache container will be named
"abc-cache-1".  The autograder may use a prefix other than p2 and
will modify the build commands accordingly.

Web requests to the caching layer specify a zipcode, and the number of
addresses that should be returned (the "limit").  To find the answer,
cache containers will ask a dataset container via gRPC.  Requests will
alternate between the two dataset containers to balance the load.  If
one dataset server is down, temporarily or long run, the cache server
should attempt to use the other dataset server to obtain the result.

**Hint:** think about whether there is any .sh script that will help you quickly test code changes.  For example, you may want it to rebuild your Dockerfiles, cleanup an old Compose cluster, and deploy a new cluster.

## Part 1: gRPC Server (Dataset Layer)

Define an RPC service called "PropertyLookup" in a .proto file.  It
should have a single RPC call named "LookupByZip".  This method should
accept a `zip` and `limit` (both int32 values) and return addresses in
a "repeated string" field.

A "dataset.py" server program should override `PropertyLookupServicer`
and start serving with the following code:

```python
server = grpc.server(futures.ThreadPoolExecutor(max_workers=1), options=[("grpc.so_reuseport", 0)])
# TODO: add servicer
server.add_insecure_port("0.0.0.0:5000")
server.start()
server.wait_for_termination()
```

The server should read Madison addresses from "addresses.csv.gz" (downloaded from https://data-cityofmadison.opendata.arcgis.com/datasets/a72d02a4fda34327ae68dd0c2fd07455_20/explore) prior to the first request so it is ready to return addresses.  Given a zipcode, it should return "limit" number of addresses (return the first ones according to an alphanumeric sort).

**AI Requirement**: use `/ask` to ask Aider about different approaches you could choose from to read the dataset.  Comment in `ai.md` about what you chose, and why.

Create a Dockerfile.dataset that lets you build a Docker image with your code
and any necessary resources.  Note that we won't install any Python
packages (such as the gRPC tools) on our test VM, so it is important
that compiling your .proto file is one of the steps that happens
during Docker build.  Your Dockerfile should also directly copy in the
dataset at build time.

**AI Requirement**: use Aider to write a simple program to test your dataset.py server before moving on.  Read the program generated, and briefly describe what is being tested in `ai.md` before proceeding.

## Part 2: HTTP Server (Cache Layer)

Create an HTTP server in a "cache.py" file.  You can do this with the
help of the Flask framework: https://flask.palletsprojects.com/en/stable/.
Here is some starter code you can use:

```python
import flask
from flask import Flask

app = Flask("p2")

@app.route("/lookup/<zipcode>")
def lookup(zipcode):
    zipcode = int(zipcode)
    limit = flask.request.args.get("limit", default=4, type=int)
    return flask.jsonify({"addrs": ["TODO"], "source": "TODO", "error": None})

def main():
    app.run("0.0.0.0", port=8080, debug=False, threaded=False)

if __name__ == "__main__":
    main()
```

Extend the above code so that it makes gRPC calls to a dataset server
to get real addresses to return back.  Note that the Docker compose
file passes in a "PROJECT" environment variable that you can access
via `os.environ`.  When you deploy server.py in a Docker container
with the help of compose, the two dataset servers will be reachable at
"\<PROJECT\>-dataset-1:5000" and "\<PROJECT\>-dataset-2:5000", so you can
create the gRPC channels/stubs accordingly in cache.py.

Your cache.py program should alternate between sending requests to
dataset server 1 or 2 in order to balance load (the first request
should go to server 1).  In the "source" field of the returned JSON
value, return "1" or "2" to indicate to a client which dataset server
cache.py relied on to obtain the answer.


Similar to Part 1, create a `Dockerfile.cache` that lets you build a Docker
image with your code and any necessary resources. Again, it is important to
install `gRPC`.

## Part 3: Retry

When a dataset server is down, your code in cache.py using the stub
will throw a `grpc.RpcError` exception.  When this happens, sleep
100ms, then try the other server.  If there are more failures, just
keep alternating, up to 5 times total.  At that point, specify an
informative string in the "error" field of the JSON being returned
(you can decide what it is, but one approach would be to convert the
exception to a string).

## Part 4: Caching

Imlement a cache in "cache.py" so that your caching server can
sometimes respond to HTTP requests without making a gRPC call to a
dataset server.

Specifications:
* implement an LRU cache of size 3
* a cache entry should consist of a zipcode and 8 corresponding addresses
* if an HTTP request specifies a limit <8 and there IS a corresponding cache entry, just slice the cache entry to get the desired number of addresses
* if an HTTP request specifies a limit <8 and there IS NOT a corresponding cache entry, request 8 addresses from the dataset server anyway so we can create a cache entry useful for subsequent requests
* if an HTTP request specifies a limit >8, we will not be able to use the cache to respond to the request, but you should still add the first 8 addresses to the cache (if not already present)
* caching should allow the HTTP servers to continue to function in a limited capacity even if all the dataset servers are down
* the "source" entry should be "cache" (no gRPC call necessary), or "1" or "2" (got the data from a dataset server)

**AI Requirement**: ask Aider for suggestions to make your code more efficient, and comment about this in `ai.md`.

## Submission

Read the directions [here](../projects.md) about how to create the
repo.

Please add `.aider.input.history`, and `./.aider.chat.history.md` to
your repo, and fill in the `ai.md` file.  Otherwise you have some
flexibility about how you write your code, but we must be able to run
it like this:

```
docker build . -f Dockerfile.cache -t p2-cache
docker build . -f Dockerfile.dataset -t p2-dataset
docker compose up -d
```

We will copy in the "docker-compose.yml" and "addresses.csv.gz" files,
overwriting anything you might have changed.

## Tester

Use the **autobadger** tool on your machine to run tests against your code:

```bash
autobadger --project=p2 --verbose
```

The `--verbose` flag will print more information to the console as your tests are running.

Pushing to `main` will submit your project and we will grade your code
on `main` from a remote VM. A GitLab issue should be pushed to your
repository shortly after you submit.
